import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

warnings.filterwarnings('ignore')
cust_df = pd.read_csv('./train.csv', encoding="latin-1")

cust_df['var3'].replace(-999999,2,inplace=True)
cust_df.drop('ID',axis=1,inplace=True)

X_features = cust_df.iloc[:,:-1]
y_labels = cust_df.iloc[:,-1]

X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0)

X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)

lgbm_clf = LGBMClassifier(n_estimators=500)

eval_set = [(X_tr, y_tr),(X_val, y_val)]
lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric="auc", eval_set=eval_set)

lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1])
print("ROC AUC : ", lgbm_roc_score)

# Hyperopt를 사용하여 베이지안 최적화로 XGBoost 하이퍼 파라미터 튜닝 수행하기
from hyperopt import hp, fmin, tpe, Trials
from sklearn.model_selection import KFold

lgbm_search_space = { 'num_leaves' : hp.quniform('num_leaves',32,64,1),
                    'max_depth': hp.quniform("max_depth",100,160,1),
                    'min_child_samples' : hp.quniform('min_child_samples',60,100,1),
                    'subsample' : hp.uniform('subsample',0.7, 1),
                    'learning_rate' : hp.uniform('learning_rate',0.01,0.2)}

def objective_func(search_space):
  lgbm_clf = LGBMClassifier(n_estimators=100, num_leaves = int(search_space['num_leaves']),
                            max_depth=int(search_space['max_depth']),
                            min_child_samples=int(search_space['min_child_samples']),
                            subsample=search_space['subsample'],
                            learning_rate = search_space['learning_rate'])
  roc_auc_list = []

  kf = KFold(n_splits=3)

  for tr_index, val_index in kf.split(X_train):
    X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
    X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]

    lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric="auc", eval_set=[(X_tr, y_tr), (X_val, y_val)])

    score = roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:,1])
    roc_auc_list.append(score)

  return (-1) * np.mean(roc_auc_list)

trials = Trials()

best = fmin(fn=objective_func, space=lgbm_search_space, algo=tpe.suggest, max_evals=50, trials=trials)
print("best params : ", best)


lgbm_clf = LGBMlassifier(n_esitmators=500, num_leaves=int(best['num_leaves']),
                         max_depth=int(best['max_depth']),
                         min_child_samples=int(best['min_child_samples']),
                         subsample=round(best['subsample'],5),
                         learning_rate = round(best['learning_rate'],5)
                         )

lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds = 100, eval_metric="auc", eval_set=[(X_tr, y_tr),(X_val, y_val)])

lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1])
print("ROC AUC : ", np.round(lgbm_roc_score))
