# This function returns some kind of performance indexes for classifiers
# Input labels, prediction result(class and probability)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
  confusion = confusion_matrix(y_test, pred)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  roc_auc = roc_auc_score(y_test, pred_proba)
  print("Confusion Matrix!")
  print(confusion)
  print("Accuracy : ", accuracy)
  print("Precision : ", precision)
  print("Recall : ", recall)
  print("F1 : ", f1)
  print("AUC : ", roc_auc)

# This function receives classifier model, Train / Test datasets and print classifier's all performance indexes on the console.
def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
  model.fit(ftr_train, tgt_train)
  pred = model.predict(ftr_test)
  pred_proba = model.predict_proba(ftr_test)[:,1]
  get_clf_eval(tgt_test, pred, pred_proba)
