import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline

import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)

bike_df = pd.read_csv("./train.csv")

bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)

bike_df['year'] = bike_df['datetime'].apply(lambda x:x.year)
bike_df['month'] = bike_df['datetime'].apply(lambda x:x.month)
bike_df['day'] = bike_df['datetime'].apply(lambda x:x.day)
bike_df['hour'] = bike_df['datetime'].apply(lambda x:x.hour)

drop_columns = ['datetime','casual','registered']
bike_df.drop(drop_columns, axis=1, inplace=True)


# 피쳐 값에 따른 count의 분포 파악하기
fig, axs = plt.subplots(figsize=(16,8), ncols=4, nrows=2)
cat_features = ['year','month','season','weather','day','hour','holiday','workingday']
for idx, feature in enumerate(cat_features):
  row = idx // 4
  col = idx % 4
  sns.barplot(x=feature, y='count', data=bike_df, ax=axs[row][col])


## 
from sklearn.metrics import mean_squared_error, mean_absolute_error

def rmsle(y, pred):
  log_y = np.log1p(y)
  log_pred = np.log1p(pred)
  squared_error = (log_y - log_pred) ** 2
  rmsle = np.sqrt(np.mean(squared_error))
  return rmsle

def rmse(y, pred):
  return np.sqrt(mean_squared_error(y, pred))

def evaluate_regr(y, pred):
  rmsle_val = rmsle(y, pred)
  rmse_val = rmse(y, pred)
  mae_val = mean_absolute_error(y, pred)
  print("RMSLE : ", rmsle_val)
  print("RMSE : ", rmse_val)
  print("MAE : ", mae_val)

def get_top_error_data(y_test, pred, n_top=5):
  result_df = pd.DataFrame(y_test.values, columns=['real_count'])
  result_df['predicted_count'] = np.round(pred)
  result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])

  print(result_df.sort_values('diff', ascending=False)[:n_top])

def get_model_predict(model, X_train, X_test,y_train, y_test, is_expm1=False):
  model.fit(X_train, y_train)
  pred = model.predict(X_test)
  if is_expm1:
    y_test = np.expm1(y_test)
    pred = np.expm1(pred)
  print('###',model.__class__.__name__,'###')
  evaluate_regr(y_test, pred)
  
  
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso

y_target = bike_df['count']

# standardize target values by applying log1p. method
y_target_log = np.log1p(y_target)

X_features = bike_df.drop('count', axis=1, inplace=False)
X_features_ohe = pd.get_dummies(X_features, columns=['year','month','day','hour','holiday','workingday','season','weather'])


X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3)

lr_reg = LinearRegression()
ridge_reg = Ridge(alpha=10)
lasso_reg = Lasso(alpha=0.01)

for model in [lr_reg, ridge_reg, lasso_reg]:
  get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)
